{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network for Regression\n",
    "In the previous homework you implemented a linear regression network. In this exercise, we will solve the same problem with a neural network instead, to leverage the power of Deep Learning.\n",
    "\n",
    "We will implement our neural networks using a modular approach. For each layer we will implement a `forward` and a `backward` function. The `forward` function will receive inputs, weights, and other parameters and will return both an output and a `cache` object storing data needed for the backward pass, like this:\n",
    "\n",
    "```python\n",
    "def layer_forward(x, w):\n",
    "  \"\"\" Receive inputs x and weights w \"\"\"\n",
    "  # Do some computations ...\n",
    "  z = # ... some intermediate value\n",
    "  # Do some more computations ...\n",
    "  out = # the output\n",
    "   \n",
    "  cache = (x, w, z, out) # Values we need to compute gradients\n",
    "   \n",
    "  return out, cache\n",
    "```\n",
    "\n",
    "The backward pass will receive upstream derivatives and the `cache` object, and will return gradients with respect to the inputs and weights, like this:\n",
    "\n",
    "```python\n",
    "def layer_backward(dout, cache):\n",
    "  \"\"\"\n",
    "  Receive derivative of loss with respect to outputs and cache,\n",
    "  and compute derivative with respect to inputs.\n",
    "  \"\"\"\n",
    "  # Unpack cache values\n",
    "  x, w, z, out = cache\n",
    "  \n",
    "  # Use values in cache to compute derivatives\n",
    "  dx = # Derivative of loss with respect to x\n",
    "  dw = # Derivative of loss with respect to w\n",
    "  \n",
    "  return dx, dw\n",
    "```\n",
    "\n",
    "After implementing a bunch of layers this way, we will be able to easily combine them to build networks with different architectures.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# As usual, a bit of setup\n",
    "\n",
    "from exercise_code.data.csv_dataset import CSVDataset\n",
    "from exercise_code.data.csv_dataset import FeatureSelectorAndNormalizationTransform\n",
    "from exercise_code.data.dataloader import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load your data\n",
    "We apply the same dataloading and preprocessing steps as in the previous exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = 'SalePrice'\n",
    "i2dl_exercises_path = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "root_path = os.path.join(i2dl_exercises_path, \"datasets\", 'housing')\n",
    "housing_file_path = os.path.join(root_path, \"housing_train.csv\")\n",
    "download_url = 'https://cdn3.vision.in.tum.de/~dl4cv/housing_train.zip'\n",
    "\n",
    "# Always make sure this line was run at least once before trying to\n",
    "# access the data manually, as the data is downloaded in the \n",
    "# constructor of CSVDataset.\n",
    "train_dataset = CSVDataset(target_column=target_column, root=root_path, download_url=download_url, mode=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train_dataset.df\n",
    "target_column = 'SalePrice'\n",
    "# Select only 2 features to keep plus the target column.\n",
    "selected_columns = ['GrLivArea','GarageArea', target_column]\n",
    "mn, mx, mean = df.min(), df.max(), df.mean()\n",
    "\n",
    "column_stats = {}\n",
    "for column in selected_columns:\n",
    "    crt_col_stats = {'min' : mn[column],\n",
    "                     'max' : mx[column],\n",
    "                     'mean': mean[column]}\n",
    "    column_stats[column] = crt_col_stats    \n",
    "\n",
    "transform = FeatureSelectorAndNormalizationTransform(column_stats, target_column)\n",
    "\n",
    "def rescale(data, key = \"SalePrice\", column_stats = column_stats):\n",
    "    \"\"\" Rescales input series y\"\"\"\n",
    "    mx = column_stats[key][\"max\"]\n",
    "    mn = column_stats[key][\"min\"]\n",
    "\n",
    "    return data * (mx - mn) + mn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 876\n",
      "Number of validation samples: 292\n",
      "Number of test samples: 292\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CSVDataset(mode=\"train\", target_column=target_column, root=root_path, download_url=download_url, transform=transform)\n",
    "val_dataset = CSVDataset(mode=\"val\", target_column=target_column, root=root_path, download_url=download_url, transform=transform)\n",
    "test_dataset = CSVDataset(mode=\"test\", target_column=target_column, root=root_path, download_url=download_url, transform=transform)\n",
    "\n",
    "print(\"Number of training samples:\", len(train_dataset))\n",
    "print(\"Number of validation samples:\", len(val_dataset))\n",
    "print(\"Number of test samples:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Build your Model\n",
    "Now we want to build our model. But let's first construct the building blocks we want to use. We will define the forward and backward pass for an affine layer and a Sigmoid activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Affine Layer\n",
    "\n",
    "Open the file `exercise_code/networks/layer.py` and implement the `affine_forward` and the `affine_backward` function. Remember, and affine layer computes a function of\n",
    "\n",
    "$$\\mathbf{z} = \\mathbf{W} \\mathbf{x}$$\n",
    "\n",
    "To check the correctness of your implementation, we will again use numeric gradient checking:\n",
    "\n",
    "$$ \\frac {df(x)}{dx} = \\frac{f(x+h) - f(x-h)}{2h} $$\n",
    "\n",
    "Once you are done you can test your implementaion by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AffineForwardTest passed.\n",
      "AffineBackwardTestDx passed.\n",
      "AffineBackwardTestDw passed.\n",
      "AffineBackwardTestDb passed.\n",
      "Congratulations you have passed all the unit tests!!! Tests passed: 4/4\n",
      "(0, 4)\n"
     ]
    }
   ],
   "source": [
    "# Test the affine function\n",
    "from exercise_code.tests.layer_tests import *\n",
    "print(AffineLayerTest()())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Sigmoid layer:\n",
    "Implement the forward pass for the sigmoid activation function in the `sigmoid_forward` function and the backward pass in `sigmoid_backward`.\n",
    "\n",
    "$$ y = \\sigma(z) = \\frac{1}{1+\\mathrm{exp}(-z)}, $$\n",
    "\n",
    "Test your implementation using the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SigmoidForwardTest passed.\n",
      "SigmoidBackwardTest passed.\n",
      "Congratulations you have passed all the unit tests!!! Tests passed: 2/2\n",
      "(0, 2)\n"
     ]
    }
   ],
   "source": [
    "# Test the sigmoid function\n",
    "\n",
    "print(SigmoidTest()())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Two-layer regression network\n",
    "Now that you have all the necessary building block, let's build your first neural network.\n",
    "\n",
    "Open the file `exercise_code/networks/regression_net.py` and complete the implementation of the `RegressionNet` class. Specifically, you again need co complete the `forward` and `backward` functions. You can run the cell below to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.21306861, 0.83998691],\n",
       "       [0.47049331, 0.33622153],\n",
       "       [0.6606928 , 0.38681508]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = 3\n",
    "hidden_size = 2\n",
    "a = np.random.randn(input_size, hidden_size)\n",
    "out = 1 / (1 + np.exp(-a))\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RegressionNetForwardTest passed.\n",
      "Method forward() correctly implemented. Tests passed: 1/1\n",
      "RegressionNetBackwardTest passed.\n",
      "Class RegressionNet correctly implemented. Tests passed: 2/2\n",
      "Score: 100/100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from exercise_code.tests.regression_net_tests import test_regression_net\n",
    "from exercise_code.networks.regression_net import RegressionNet\n",
    "test_regression_net(RegressionNet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Optimizer & Loss Function\n",
    "We have now implemented:\n",
    "- [x]  A dataloader\n",
    "- [x]  A loss function\n",
    "- [x]  A model\n",
    "- [ ]  An optimizer\n",
    "- [ ]  A loss function\n",
    "\n",
    "The only things missing in out Deep Learning pipeline are an optimizer and a loss function. Since you already implemented SGD and MSE in last weeks' exercise, we will give them to you this time. Have a look at their implementations in `exercise_code/networks/optimizer.py` and `exercise_code/networks/loss.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.networks.optimizer import SGD\n",
    "from exercise_code.networks.loss import MSE, L1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Solver\n",
    "Now that we have everything together, let's update our solver from exercise_04 and finally start training our model.\n",
    "\n",
    "Open the file `exercise_code/solver.py` and read through it to familiarize yourself with the API. In the `train` and `_step` functions, you can see all of the components you implemented in the last exercises working together. Now, run the solver to train your model.\n",
    "\n",
    "We provide you with a default set of hyperparameters here as hyperparameter search is not the scope of this exercise. However, you can still play around with those values and see how the training performance changes. Especially the `std` which is the standard deviation of the gaussian distribution used to initialize the weights of your model is very sensitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 loss on test set BEFORE training: 1,686,081\n",
      "MSE loss on test set BEFORE training: 2,866,170,408,126\n",
      "(Epoch 1 / 20) train loss: 5.561933; val loss: 5.606161\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "non-broadcastable output operand with shape (100,) doesn't match the broadcast shape (1,100)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-a1561adf4a0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0my_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Introduction-to-DL/exercise_05/exercise_code/solver.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs)\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0;31m# Update the model parameters.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0mvalidate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m                 \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_batch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Introduction-to-DL/exercise_05/exercise_code/solver.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, X, y, validation)\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;31m# Update weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0;31m# If it was a training step, we need to count operations for\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Introduction-to-DL/exercise_05/exercise_code/networks/optimizer.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;31m# Update the parameter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0mw_updated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw_updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Introduction-to-DL/exercise_05/exercise_code/networks/optimizer.py\u001b[0m in \u001b[0;36m_update\u001b[0;34m(self, w, dw, lr)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mUpdate\u001b[0m \u001b[0ma\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mparameter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \"\"\"\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mw\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: non-broadcastable output operand with shape (100,) doesn't match the broadcast shape (1,100)"
     ]
    }
   ],
   "source": [
    "from exercise_code.networks.regression_net import RegressionNet\n",
    "from exercise_code.solver import Solver\n",
    "\n",
    "batch_size = 4\n",
    "lr = 1e-3\n",
    "hidden_size = 100\n",
    "std = 1.\n",
    "epochs = 20\n",
    "\n",
    "model = RegressionNet(input_size=2, hidden_size=hidden_size, std=std)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "\n",
    "solver = Solver(model, train_dataloader, val_dataloader, learning_rate=lr, loss_func=MSE(), optimizer=SGD)\n",
    "\n",
    "# add test data to test before training\n",
    "X_test = [test_dataset[i]['features'] for i in range((len(test_dataset)))]\n",
    "X_test = np.stack(X_test, axis=0)\n",
    "y_test = [test_dataset[i]['target'] for i in range((len(test_dataset)))]\n",
    "y_test = np.stack(y_test, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "y_out = solver.get_dataset_prediction(test_dataloader)\n",
    "l1_loss = L1()\n",
    "mse_loss = MSE()\n",
    "print(\"L1 loss on test set BEFORE training: {:,.0f}\".format(l1_loss(rescale(y_out), rescale(y_test))[0].mean() ))\n",
    "print(\"MSE loss on test set BEFORE training: {:,.0f}\".format(mse_loss(rescale(y_out), rescale(y_test))[0].mean() ))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "solver.train(epochs=epochs)\n",
    "\n",
    "y_out, _ = model(X_test)\n",
    "l1_loss = L1()\n",
    "mse_loss = MSE()\n",
    "print(\"L1 loss on test set AFTER training: {:,.0f}\".format(l1_loss(rescale(y_out), rescale(y_test))[0].mean() ))\n",
    "print(\"MSE loss on test set AFTER training: {:,.0f}\".format(mse_loss(rescale(y_out), rescale(y_test))[0].mean() ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run this cell to visualize your training and validation loss and your prediction\n",
    "y_out = solver.get_dataset_prediction(test_dataloader)\n",
    "\n",
    "\n",
    "plt.title('Loss curves')\n",
    "plt.plot(solver.train_loss_history, '-', label='train')\n",
    "plt.plot(solver.val_loss_history, '-', label='val')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel('Iteration')\n",
    "plt.show()\n",
    "\n",
    "if np.shape(X_test)[1]==1:\n",
    "    plt.scatter(X_test, y_test, label = \"Ground Truth\")\n",
    "    inds = X_test.argsort(0).flatten()\n",
    "    plt.plot(X_test[inds], y_out[inds], color='r', label = \"Prediction\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "else:\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "    fig = plt.figure()\n",
    "    ax = plt.axes(projection='3d')\n",
    "    first_feature = X_test[:, 0]\n",
    "    second_feature = X_test[:, 1]\n",
    "    salePrice = y_test[:, 0]\n",
    "    \n",
    "    salePricePred = y_out[:, 0]\n",
    "    ax.plot_trisurf(first_feature, second_feature, salePricePred, linewidth=0, antialiased=True,color =\"red\")\n",
    "\n",
    "    ax.scatter(first_feature, second_feature, salePrice)\n",
    "  \n",
    "    ax.set_xlabel(selected_columns[0])\n",
    "    ax.set_ylabel(selected_columns[1])\n",
    "    ax.set_zlabel(selected_columns[2])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model for submission\n",
    "\n",
    "Simply save your objects using the following cell. This will save them to a pickle file `models/two_layer_regression.p`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.tests import save_pickle\n",
    "\n",
    "save_pickle(\n",
    "    data_dict={\n",
    "        \"Regression_Net\": RegressionNet\n",
    "    },\n",
    "    file_name=\"two_layer_regression.p\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission Goals\n",
    "\n",
    "- Goal: Successfully implement the forward and backward pass of a two layer regression neural network\n",
    "- Test cases:\n",
    "  1. Does `forward()` and `backward()` of your 2 layer regression neural net return the correct value and data type?\n",
    "- Reachable points [0, 100]: 0 if not implemented, 100 if all tests passed, 50 per passed test\n",
    "- Threshold to clear exercise: 100\n",
    "- Submission start: __May 22, 2020 12.00__\n",
    "- Submission deadline : __May 27, 2020 23.59__ \n",
    "- You can make multiple submission uptil the deadline. Your __best submission__ will be considered for bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
