{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Introduction\n",
    "\n",
    "This is an introduction of PyTorch. It’s a Python-based scientific computing package targeted at two sets of audiences:\n",
    "\n",
    "- A replacement for NumPy to use the power of GPUs;\n",
    "\n",
    "- a deep learning research platform that provides maximum flexibility and speed.\n",
    "    - [`torch.Tensor`](https://pytorch.org/docs/stable/tensors.html) is the central class of PyTorch.\n",
    "\n",
    "    - Central to all neural networks in PyTorch is the [`autograd`](https://pytorch.org/docs/stable/autograd.html)\n",
    "    package. It provides automatic differentiation for all\n",
    "    operations on Tensors. If we set the attribute `.requires_grad` of `torch.Tensor` as `True`, it starts to\n",
    "    track all operations on it. When finishing computation, we can call `.backward()` and have all the gradients\n",
    "    computed automatically. The gradient for this tensor will be accumulated into `.grad` attribute.\n",
    "\n",
    "\n",
    "## Goals of this tutorial\n",
    "\n",
    "- Understanding PyTorch's Tensor library and neural networks at a high level;\n",
    "\n",
    "- Training a small network with PyTorch;\n",
    "\n",
    "\n",
    "## Preparation\n",
    "\n",
    "- Install [PyTorch](https://pytorch.org/) and [torchvision](https://github.com/pytorch/vision) (CPU version); (**If you want to install a cuda version, remember to change the type of the following cell into markdown**)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install torch==1.4.0+cpu torchvision==0.5.0+cpu -f https://download.pytorch.org/whl/torch_stable.html > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Requirement already satisfied: torch==1.4.0+cu92 in /home/yuxuan/anaconda3/lib/python3.8/site-packages (1.4.0+cu92)\n",
      "Requirement already satisfied: torchvision==0.5.0+cu92 in /home/yuxuan/anaconda3/lib/python3.8/site-packages (0.5.0+cu92)\n",
      "Requirement already satisfied: numpy in /home/yuxuan/anaconda3/lib/python3.8/site-packages (from torchvision==0.5.0+cu92) (1.18.5)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/yuxuan/anaconda3/lib/python3.8/site-packages (from torchvision==0.5.0+cu92) (7.2.0)\n",
      "Requirement already satisfied: six in /home/yuxuan/anaconda3/lib/python3.8/site-packages (from torchvision==0.5.0+cu92) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#cuda 9.2 for pytorch 1.4.0 and torchvision 0.5.0\n",
    "\n",
    "pip install torch==1.4.0+cu92 torchvision==0.5.0+cu92 -f https://download.pytorch.org/whl/torch_stable.html "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- <div class=\"alert alert-block alert-info\"><b>(Optional)</b> You can also install a\n",
    "<a href=\"https://developer.nvidia.com/cuda-downloads\">Cuda</a>\n",
    "version if an Nvidia GPU and Cuda setup is installed on your machine, e.g.</div>\n",
    "\n",
    "```python\n",
    "# CUDA 10.1\n",
    "pip install torch==1.4.0+cu101 torchvision==0.5.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "```\n",
    "- <div class=\"alert alert-block alert-danger\">Make sure you've installed the <b>same version of PyTorch and\n",
    " torchvision</b>. If you install your own version, there might be some issues.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 1.4.0+cu92\n",
      "Torchvision version: 0.5.0+cu92\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "print(f\"Torch version: {torch.__version__}\\nTorchvision version: {torchvision.__version__}\\n\")\n",
    "if not torch.__version__.startswith(\"1.4.0\"):\n",
    "    print(\"you are using an another version of PyTorch. We expect PyTorch 1.4.0. You can continue with your version but it\"\n",
    "          \" might cause some issues\")\n",
    "if not torchvision.__version__.startswith(\"0.5.0\"):\n",
    "    print(\"you are using an another version of torchvision. We expect torchvision 0.5.0. You can continue with your version but it\"\n",
    "          \" might cause some issues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1. Getting Started\n",
    "\n",
    "In this session you will learn the basic element Tensor and some simple oprations of PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.1 Tensors\n",
    "\n",
    "Tensors are similar to NumPy’s ndarrays, with the addition being that Tensors can also be used on a GPU to accelerate\n",
    "computing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_np:\n",
      " <class 'numpy.ndarray'>,\n",
      " Shape: (2, 3)\n",
      "[[1 2 3]\n",
      " [5 6 7]]\n",
      "a_ts:\n",
      " <class 'torch.Tensor'>,\n",
      " Shape: torch.Size([2, 3])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n"
     ]
    }
   ],
   "source": [
    "# Construct a (2,3) NumPy array and a (2,3) tensor directly from data\n",
    "# [[1 2 3]\n",
    "#  [4 5 6]]\n",
    "a_np = np.array([[1,2,3],[5,6,7]]) #NumPy array\n",
    "a_ts = torch.tensor([[1,2,3],[4,5,6]]) # Tensor\n",
    "print(\"a_np:\\n {},\\n Shape: {}\".format(type(a_np), a_np.shape))\n",
    "print(a_np)\n",
    "print(\"a_ts:\\n {},\\n Shape: {}\".format(type(a_ts), a_ts.shape)  )\n",
    "print(a_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.2 Conversion btw. NumPy ndarray and Tensor\n",
    "\n",
    "The conversion between NumPy ndarray and PyTorh tensor is quite easy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Conversion\n",
    "m_np = np.array([1, 2, 3])\n",
    "n_ts = torch.from_numpy(m_np) #Convert a numpy array to a Tensor\n",
    "\n",
    "v_np = n_ts.numpy() #Tensor to numpy\n",
    "v_np[1] = -1 ############important: Numpy and Tensor share the same memory!!!!!!\n",
    "assert(m_np[1] == v_np[1]) #Change Numpy will also change the Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>Hint:</b> During the conversion, both ndarray and Tensor share the same memory storage. Change value from either side will\n",
    "affect the other.</div>\n",
    "\n",
    "### 1.3 Operations\n",
    "\n",
    "#### 1.3.1 Indexing\n",
    "\n",
    "We can use the NumPy indexing in Tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 2],\n",
      "        [0, 5]])\n"
     ]
    }
   ],
   "source": [
    "# Let us take the first two columns from the original array and save it in a new one\n",
    "b = a_ts[:2, :2] #Use numpy type indexing\n",
    "#b.shape\n",
    "b[:, 0] = 0 #For assignment\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 3, 5, 6])\n"
     ]
    }
   ],
   "source": [
    "# Select elements which satisfy a condition\n",
    "# Using numpy array makes such a selection trivial\n",
    "mask = a_ts > 1\n",
    "new_array = a_ts[mask] #The element in a_ts which is greater than 1\n",
    "print(new_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "# Do the same thing in a single step\n",
    "c = a_ts[a_ts>1]\n",
    "print(c == new_array) #Why assert doesn't work here\n",
    "##assert np.all(new_array == c) #  np.all() to indicate that all the values need to match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 1.3.2 Mathematical operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x + y: tensor([[ 6,  8],\n",
      "        [10, 12]])\n",
      "x + y: tensor([[ 6,  8],\n",
      "        [10, 12]])\n",
      "x + y: tensor([[ 6.,  8.],\n",
      "        [10., 12.]])\n",
      "x - y: tensor([[-4, -4],\n",
      "        [-4, -4]])\n",
      "x - y: tensor([[-4, -4],\n",
      "        [-4, -4]])\n",
      "x - y: tensor([[-4., -4.],\n",
      "        [-4., -4.]])\n",
      "x * y: tensor([[ 5, 12],\n",
      "        [21, 32]])\n",
      "x * y: tensor([[ 5, 12],\n",
      "        [21, 32]])\n",
      "x * y: tensor([[ 5., 12.],\n",
      "        [21., 32.]])\n"
     ]
    }
   ],
   "source": [
    "# Mathematical operations\n",
    "x = torch.tensor([[1,2],[3,4]])\n",
    "y = torch.tensor([[5,6],[7,8]])\n",
    "\n",
    "# Elementwise Addition\n",
    "# [[ 6.0  8.0]\n",
    "#  [10.0 12.0]]\n",
    "#Addition: syntax 1\n",
    "print(\"x + y: {}\".format(x + y))\n",
    "#Addition: syntax 2\n",
    "print(\"x + y: {}\".format(torch.add(x, y)))\n",
    "#Addition: syntax 3\n",
    "result_add = torch.empty(2, 2)\n",
    "torch.add(x, y, out=result_add)\n",
    "print(\"x + y: {}\".format(result_add))\n",
    "\n",
    "# Elementwise Subtraction\n",
    "# [[-4.0 -4.0]\n",
    "#  [-4.0 -4.0]]\n",
    "# Subtraction: syntax 1\n",
    "print(\"x - y: {}\".format(x - y))\n",
    "# Subtraction: syntax 2\n",
    "print(\"x - y: {}\".format(torch.sub(x, y)))\n",
    "# Subtraction: syntax 3\n",
    "result_sub = torch.empty(2, 2)\n",
    "torch.sub(x, y, out=result_sub)\n",
    "print(\"x - y: {}\".format(result_sub))\n",
    "\n",
    "# Elementwise Multiplication\n",
    "# [[ 5.0 12.0]\n",
    "#  [21.0 32.0]]\n",
    "# Multiplication: syntax 1\n",
    "print(\"x * y: {}\".format(x * y))\n",
    "# Multiplication: syntax 2\n",
    "print(\"x * y: {}\".format(torch.mul(x, y)))\n",
    "# Multiplication: syntax 3\n",
    "result_mul = torch.empty(2, 2)\n",
    "torch.mul(x, y, out=result_mul)\n",
    "print(\"x * y: {}\".format(result_mul))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "When dividing two ints in NumPy, the result is always a **float**, e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.2        0.33333333]\n",
      " [0.42857143 0.5       ]]\n"
     ]
    }
   ],
   "source": [
    "x_np = np.array([[1,2],[3,4]])\n",
    "y_np = np.array([[5,6],[7,8]])\n",
    "print(x_np / y_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "\n",
    "**However, in PyTorch 1.4.0 `torch.div` calculates floor division if both operands have integer types**;\n",
    "  If you want **true division** for integers, pleases convert the integers into floats first or specify the output as\n",
    "  `torch.div(a, b, out=c)`.\n",
    "<div class=\"alert alert-block alert-danger\">In PyTorch 1.5.0 you can use <b>true_divide</b> or <b>floor_divide</b>\n",
    " to calculate true division or floor division. And in future release div will perform true division as in Python 3. </div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x // y: tensor([[0, 0],\n",
      "        [0, 0]])\n",
      "x // y: tensor([[0, 0],\n",
      "        [0, 0]])\n",
      "x / y: tensor([[0.2000, 0.3333],\n",
      "        [0.4286, 0.5000]])\n"
     ]
    }
   ],
   "source": [
    "# Elementwise Division\n",
    "# Floor Division: syntax 1\n",
    "print(\"x // y: {}\".format(x / y))\n",
    "# Floor Division: syntax 2\n",
    "print(\"x // y: {}\".format(torch.div(x, y)))\n",
    "# True Division: syntax 1\n",
    "result_true_div = torch.empty(2, 2)\n",
    "torch.div(x, y, out=result_true_div)\n",
    "print(\"x / y: {}\".format(result_true_div))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Devices\n",
    "\n",
    "When training a neural network, make sure that all the tensors are on the same device. Tensors can be moved onto any device using `.to` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Original device: cpu\n",
      "Current device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# We will use ``torch.device`` objects to move tensors in and out of GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "print(f\"Original device: {x.device}\") # \"cpu\", integer\n",
    "\n",
    "tensor = x.to(device)\n",
    "print(f\"Current device: {tensor.device}\") #\"cpu\" or \"cuda\", double"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So `x` has been moved onto cuda for those who have a GPU; otherwise it's still on the CPU.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><b>Tip:</b> Include the <b>.to(device)</b> calls for every project such that\n",
    "you can easily port it to a GPU version.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. Training a classifier with PyTorch\n",
    "\n",
    "In this session, you'll have an overview about how we could use PyTorch to load data, define neural networks, compute\n",
    "loss and make updates to the weights of the network.\n",
    "\n",
    "\n",
    "We will do the following steps in order:\n",
    "\n",
    "a) Dataloading in Pytorch compared to our previous datasets\n",
    "\n",
    "b) Define a two-layer network\n",
    "\n",
    "c) Define a loss function and optimizer\n",
    "\n",
    "d) Train the network\n",
    "\n",
    "e) Test the network\n",
    "\n",
    "### 2.1 Datasets and Loading\n",
    "\n",
    "The general procedure of dataloading is:\n",
    "\n",
    "a) Extract: Get the data from the source\n",
    "\n",
    "b) Transform: Put our data into suitable form (e.g. tensor form)\n",
    "\n",
    "c) Load: Put our data into an object to make it easily accessible\n",
    "\n",
    "#### 2.1.1 House price\n",
    "\n",
    "We'll use our dataloader and the dataloader of PyTorch to load the house price dataset separately.\n",
    "\n",
    "First, let's initialize our csv dataset from exercise 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/91571 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://cdn3.vision.in.tum.de/~dl4cv/housing_train.zip to /home/yuxuan/Documents/IN2346_I2DL/Bonus Exercise/datasets/housing/housing_train.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98304it [00:00, 405096.42it/s]           \n"
     ]
    }
   ],
   "source": [
    "from exercise_code.data.csv_dataset import CSVDataset, get_exercise5_transform\n",
    "from exercise_code.data.dataloader import DataLoader as our_DataLoader\n",
    "\n",
    "# dataloading and preprocessing steps as in ex04 2_logistic_regression.ipynb\n",
    "target_column = 'SalePrice'\n",
    "i2dl_exercises_path = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "root_path = os.path.join(i2dl_exercises_path, \"datasets\", 'housing')\n",
    "housing_file_path = os.path.join(root_path, \"housing_train.csv\")\n",
    "download_url = 'https://cdn3.vision.in.tum.de/~dl4cv/housing_train.zip'\n",
    "\n",
    "# Set up the transform to get two prepared columns\n",
    "select_two_columns_transform = get_exercise5_transform()\n",
    "\n",
    "# Set up the dataset\n",
    "our_csv_dataset = CSVDataset(target_column=target_column, root=root_path, download_url=download_url, mode=\"train\",\n",
    "                             transform=select_two_columns_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can set up our dataloader similar to Exercise 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting item 0\n",
      "item contains\n",
      "features\n",
      "<class 'numpy.ndarray'>\n",
      "(4, 2)\n",
      "target\n",
      "<class 'numpy.ndarray'>\n",
      "(4, 1)\n"
     ]
    }
   ],
   "source": [
    "# Set up our old dataloader\n",
    "batch_size = 4\n",
    "our_dataloader = our_DataLoader(our_csv_dataset, batch_size=batch_size)\n",
    "\n",
    "for i, item in enumerate(our_dataloader):\n",
    "    print('Starting item {}'.format(i))\n",
    "    print('item contains')\n",
    "    for key in item:\n",
    "        print(key)\n",
    "        print(type(item[key]))\n",
    "        print(item[key].shape)\n",
    "    \n",
    "    if i+1 >= 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In pyTorch we can directly use a [`Dataloader` class](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)\n",
    "and simply initalize it. And it also provides more parameters than ours, such as easy multiprocessing using `num_workers`. You can refer to the link\n",
    "to learn those additional supports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting item 0\n",
      "item contains\n",
      "features\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([4, 2])\n",
      "target\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([4, 1])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "pytorch_dataloader = DataLoader(our_csv_dataset, batch_size=batch_size)\n",
    "\n",
    "# We can use the exact same way to iterate over samples\n",
    "for i, item in enumerate(pytorch_dataloader):\n",
    "    print('Starting item {}'.format(i))\n",
    "    print('item contains')\n",
    "    for key in item:\n",
    "        print(key)\n",
    "        print(type(item[key]))\n",
    "        print(item[key].shape)\n",
    "    \n",
    "    if i+1 >= 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">As you can see, both dataloaders load the data with batch_size 4 and the data contains 2 features and 1 target. The only <b>difference</b> here is that the Dataloader of PyTorch will automatically transform the dataset into tensor format.</div>\n",
    "\n",
    "#### 2.1.2 Torchvision\n",
    "\n",
    "Specifically for vision, there's a package called `torchvision`, that has data loaders for common datasets such\n",
    "as Imagenet, FashionMNIST, MNIST, etc. and data transformers for images:\n",
    "`torchvision.datasets` and `torch.utils.data.DataLoader`.\n",
    "\n",
    "This provides a huge convenience and avoids writing boilerplate code.\n",
    "\n",
    "For this tutorial, we will use FashionMNIST dataset. It has 10 classes: 'T-shirt/top', 'Trouser', 'Pullover',\n",
    "'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'. The images in FashionMNIST\n",
    "are of size $1 \\times 28 \\times 28 $, i.e. 1-channel color images of $ 28 \\times 28 $ pixels in size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ../datasets/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f85d480c74f48a593a734cdf7b612d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../datasets/FashionMNIST/raw/train-images-idx3-ubyte.gz to ../datasets/FashionMNIST/raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ../datasets/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "737ad7f6792444d2a68f29ae8f50350c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../datasets/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ../datasets/FashionMNIST/raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ../datasets/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eb7dfd0a49e442081772a9e76c26428",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../datasets/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ../datasets/FashionMNIST/raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ../datasets/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4600f4e1ead74be384d6bee5e78ff35a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../datasets/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ../datasets/FashionMNIST/raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#Define a transform to convert images to tensor\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,),(0.5,))])  # mean and std have to be sequences (e.g. tuples),\n",
    "                                                                      # therefore we should add a comma after the values\n",
    "\n",
    "fashion_mnist_dataset = torchvision.datasets.FashionMNIST(root='../datasets', train=True,\n",
    "                                                          download=True, transform=transform)\n",
    "fashion_mnist_test_dataset = torchvision.datasets.FashionMNIST(root='../datasets', train=False,\n",
    "                                                          download=True, transform=transform)\n",
    "\n",
    "fashion_mnist_dataloader = DataLoader(fashion_mnist_dataset, batch_size=8)\n",
    "fashion_mnist_test_dataloader = DataLoader(fashion_mnist_test_dataset, batch_size=8)\n",
    "\n",
    "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "           'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- `transforms.Compose` creates a series of transformation to prepare the dataset.\n",
    "\n",
    "- `transforms.ToTenser` convert `PIL image` or numpy.ndarray $(H \\times W\\times C)$ in the range [0,255] to a\n",
    "`torch.FloatTensor` of shape $(C \\times H \\times W)$ in the range [0.0, 1.0].\n",
    "\n",
    "- `transforms.Normalize` normalize a tensor image with mean and standard deviation.\n",
    "\n",
    "- `datasets.FashionMNIST` to download the Fashion MNIST datasets and transform the data.\n",
    "`train=True` if we want to get the training set; otherwise set `train=False` to get the\n",
    "test set.\n",
    "\n",
    "- `torch.utils.data.Dataloader` takes our training data or test data with parameter\n",
    "`batch_size` and `shuffle`. `batch_size` defines how many samples per batch to load.\n",
    "`shuffle=True` makes the data reshuffled at every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting item 0\n",
      "item contains\n",
      "Type of input: <class 'torch.Tensor'>\n",
      "Shape of the input: torch.Size([8, 1, 28, 28])\n",
      "label: tensor([9, 0, 0, 3, 0, 2, 7, 2])\n"
     ]
    }
   ],
   "source": [
    "# We can use the exact same way to iterate over samples\n",
    "for i, item in enumerate(fashion_mnist_dataloader):\n",
    "    print('Starting item {}'.format(i))\n",
    "    print('item contains')\n",
    "    image, label = item\n",
    "    print(f\"Type of input: {type(image)}\")\n",
    "    print(f\"Shape of the input: {image.shape}\")\n",
    "    print(f\"label: {label}\")\n",
    "\n",
    "    if i+1 >= 1:\n",
    "        break\n",
    "        \n",
    "#[8,1,28,28]: in the first batch of data loader, 8 figures, 1 chanel(gray scale), resolution 28x28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Since we loaded the data with `batch_size` 8, the shape of the input is (8, 1, 28, 28). So before we push it into the affine layer, we need to flatten it with `x = x.view(-1, x.size[0)` (It will be shown later in 2.2)\n",
    "\n",
    "\n",
    "Let's show some of the training images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ankle boot T-shirt/top T-shirt/top Dress T-shirt/top Pullover Sneaker Pullover\n"
     ]
    }
   ],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5 # unormalize\n",
    "    npimg = img.numpy() #convert the torch tensor into numpy\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(fashion_mnist_dataloader)\n",
    "images, labels = dataiter.next() #whats is the next in the iterator\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(8)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.2 Define a Two-Layer Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%% md\n"
    }
   },
   "source": [
    "In exercise_06 we've defined the forward and backward pass for an affine layer and a Sigmoid layer\n",
    "(`exercise_code/networks/layer.py`) and completed the implementation of the `ClassificationionNet` class\n",
    "(`exercise_code/networks/classifiation_net.py`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from exercise_code.networks.classification_net import ClassificationNet\n",
    "hidden_size = 100\n",
    "std = 1.0\n",
    "model_ex06 = ClassificationNet(input_size=2, hidden_size=hidden_size, std=std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Have a look at your lengthy implementation first ;). Now, we can use `torch.nn.Module` to define our network class, e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, activation=nn.Sigmoid(),\n",
    "                 input_size=1*28*28, hidden_size=100, classes=10):\n",
    "        super(Net, self).__init__()\n",
    "        self.input_size = input_size\n",
    "\n",
    "        # Here we initialize our activation and set up our two linear layers\n",
    "        self.activation = activation #activation is input of the class\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) #fc1 and fc2 are two linear layers, one in the front one in the end, sandwich\n",
    "        self.fc2 = nn.Linear(hidden_size, classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_size) # flatten each patch in [1x28x28]\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x) #sandwich structure\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Similar to the `ClassificationNet` in exercise_06, here we defined a network with PyTorch.\n",
    "\n",
    " - PyTorch provides a `nn.Module` that builds neural networks\n",
    "\n",
    " - `super().__init__` creates a class that inherits attributes and behaviors from another\n",
    " class\n",
    "\n",
    " - `self.fc1` creates an affine layer with `input_size` inputs and `hidden_size` outputs.\n",
    "\n",
    " - `self.fc2` is similar to `self.fc1`.\n",
    "\n",
    " - `Forward` pass:\n",
    "\n",
    "    - first flatten the `x` with `x = x.view(-1, self.input_size)`\n",
    "\n",
    "    - 'Sandwich layer' by applying `fc1`, `activation`, `fc2` sequentially.\n",
    "    \n",
    "<div class=\"alert alert-block alert-info\">Thanks to <b>autograd</b> package, we just have to define the <b>forward</b> function. \n",
    " And the <b>backward</b> function (where gradients are computed) is automatically defined. We can use any of the Tensor operations in the <b>forward</b>  function.</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> We can use <b>print</b> to see all difined layers (but it won't show\n",
    "the information of the forward pass).\n",
    "\n",
    "And all the learnable parameters of a model are returned by <b>[model_name].parameters()</b>. We also have access to\n",
    "the parameters of different layers by <b>[model_name].[layer_name].parameters()</b> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (activation): Sigmoid()\n",
      "  (fc1): Linear(in_features=784, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n",
      "torch.Size([100, 784])\n",
      "torch.Size([100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "#  create model\n",
    "net = Net()\n",
    "net = net.to(device) #always remember to move the network to the device\n",
    "\n",
    "print(net)\n",
    "\n",
    "for parameter in net.parameters():\n",
    "    print(parameter.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.3 Define a Loss function and optimizer\n",
    "\n",
    "Let's use a Classification Cross-Entropy loss and SGD with momentum.\n",
    "\n",
    "Recall that we've implemented SGD and MSE in exercise_04. Have a look at their implementations in\n",
    " `exercise_code/networks/optimizer.py` and `exercise_code/networks/loss.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from exercise_code.networks.optimizer import SGD\n",
    "from exercise_code.networks.loss import MSE, L1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we can import the loss function and optimizer directly from `torch.nn` and `torch.optim` respectively, e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.4 Train the network\n",
    "\n",
    "This is when things start to get interesting. We simply have to loop over our data iterator, and feed the inputs to\n",
    "the network and optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(fashion_mnist_dataset) = 60000, totally 60000 images\n",
    "#len(fashion_mnist_dataloader) = 7500, totally 7500 batches, each contains 8 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1, Iteration  1000] loss: 1.519 acc: 57.65 %\n",
      "[Epoch 1, Iteration  2000] loss: 0.888 acc: 72.21 %\n",
      "[Epoch 1, Iteration  3000] loss: 0.738 acc: 74.40 %\n",
      "[Epoch 1, Iteration  4000] loss: 0.656 acc: 76.64 %\n",
      "[Epoch 1, Iteration  5000] loss: 0.612 acc: 78.54 %\n",
      "[Epoch 1, Iteration  6000] loss: 0.578 acc: 79.31 %\n",
      "[Epoch 1, Iteration  7000] loss: 0.557 acc: 80.10 %\n",
      "[Epoch 2, Iteration  1000] loss: 0.523 acc: 81.67 %\n",
      "[Epoch 2, Iteration  2000] loss: 0.509 acc: 82.23 %\n",
      "[Epoch 2, Iteration  3000] loss: 0.512 acc: 81.94 %\n",
      "[Epoch 2, Iteration  4000] loss: 0.487 acc: 83.16 %\n",
      "[Epoch 2, Iteration  5000] loss: 0.485 acc: 83.31 %\n",
      "[Epoch 2, Iteration  6000] loss: 0.473 acc: 83.46 %\n",
      "[Epoch 2, Iteration  7000] loss: 0.473 acc: 83.45 %\n",
      "FINISH.\n"
     ]
    }
   ],
   "source": [
    "train_loss_history = [] # loss\n",
    "train_acc_history = [] # accuracy\n",
    "for epoch in range(2):\n",
    "\n",
    "    # TRAINING\n",
    "    running_loss = 0.0\n",
    "    correct = 0.0\n",
    "    total = 0\n",
    "    for i, data in enumerate(fashion_mnist_dataloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        X, y = data\n",
    "\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        y_pred = net(X) # input x and predict based on x\n",
    "        loss = criterion(y_pred, y) # calculate the loss\n",
    "        loss.backward()  # backpropagation, compute gradients\n",
    "        optimizer.step()  # apply gradients\n",
    "\n",
    "        # loss and acc\n",
    "        running_loss += loss.item()\n",
    "        _, preds = torch.max(y_pred, 1) #convert output probabilities to predicted class\n",
    "        correct += preds.eq(y).sum().item()\n",
    "        total += y.size(0)\n",
    "\n",
    "        # print statistics\n",
    "        if i % 1000 == 999: # print every 1000 mini-batches\n",
    "            running_loss /= 1000\n",
    "            correct /= total\n",
    "            print(\"[Epoch %d, Iteration %5d] loss: %.3f acc: %.2f %%\" % (epoch+1, i+1, running_loss, 100*correct))\n",
    "            train_loss_history.append(running_loss)\n",
    "            train_acc_history.append(correct)\n",
    "            running_loss = 0.0\n",
    "            correct = 0.0\n",
    "            total = 0\n",
    "\n",
    "print('FINISH.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "So the general training pass is as fowllows:\n",
    "\n",
    "- `zero_grad()`: zero the gradient buffers of all parameters and backprops with random gradient\n",
    "\n",
    "- `y_pred = net(X)`: make a forward pass through the network to getting log probabilities by passing the\n",
    "images to the model.\n",
    "\n",
    "- `loss = criterion(y_pred, y)`: calculate the loss\n",
    "\n",
    "- `loss.backward()`: perform a backward pass through the network to calculate the gradients for model parameters.\n",
    "\n",
    "-  `optimizer.step()`: take a step with the optimizer to update the model parameters.\n",
    "\n",
    "We keep tracking the training loss and accuracy over time. The following plot shows averages values for train loss and\n",
    "accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_acc_history)\n",
    "plt.plot(train_loss_history)\n",
    "plt.title(\"FashionMNIST\")\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('acc/loss')\n",
    "plt.legend(['acc', 'loss'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Test the network on the test data\n",
    "\n",
    "We have trained the network for 2 passes over the training dataset. Now we want to check\n",
    "the model by predicting the class label that the neural network outputs, and checking it\n",
    "against the ground-truth. If the prediction is correct, we add the sample to the list of\n",
    "correct predictions.\n",
    "\n",
    "And we'll visualize the data to display test images and their labels in the following format: `predicted (ground-truth)`. The text will be green for accurately classified examples and red for incorrect predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of device type cuda but got device type cpu for argument #2 'mat1' in call to _th_addmm",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-09990912c6ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# get sample outputs, apply net() on the test images to get the outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m# convert output probabilites to predicted class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-e21dfbee24ba>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# flatten each patch in [1x28x28]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#sandwich structure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1368\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of device type cuda but got device type cpu for argument #2 'mat1' in call to _th_addmm"
     ]
    }
   ],
   "source": [
    "#obtain one batch of test images\n",
    "dataiter = iter(fashion_mnist_test_dataloader)\n",
    "images, labels = dataiter.__next__()\n",
    "\n",
    "# get sample outputs, apply net() on the test images to get the outputs\n",
    "outputs = net(images)\n",
    "# convert output probabilites to predicted class\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "# prep images for display\n",
    "images = images.numpy()\n",
    "\n",
    "# plot the images in the batch, along with predicted and true labels\n",
    "fig = plt.figure(figsize=(25,4))\n",
    "for idx in range(8):\n",
    "    ax = fig.add_subplot(2, 8/2, idx+1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
    "    ax.set_title(f\"{classes[predicted[idx]]} ({classes[labels[idx]]})\",\n",
    "                color=\"green\" if predicted[idx]==labels[idx] else \"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can also show what are the classes that performed well, and the classes that did not perform well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of device type cuda but got device type cpu for argument #2 'mat1' in call to _th_addmm",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-72da9c904f11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfashion_mnist_test_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-e21dfbee24ba>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# flatten each patch in [1x28x28]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#sandwich structure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1368\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of device type cuda but got device type cpu for argument #2 'mat1' in call to _th_addmm"
     ]
    }
   ],
   "source": [
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "with torch.no_grad():\n",
    "    for data in fashion_mnist_test_dataloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(4):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    print('Accuracy of %11s: %2d %%' % (classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Reference\n",
    "\n",
    "1. [PyTorch Tutorial](https://pytorch.org/tutorials/)\n",
    "\n",
    "2. [Fashion MNIST dataset training using PyTorch](https://medium.com/@aaysbt/fashion-mnist-data-training-using-pytorch-7f6ad71e96f4)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
